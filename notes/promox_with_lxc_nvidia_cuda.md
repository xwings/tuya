Promox version: 8
Nvidia card: RTX 3060

Remove Previous Nvidia Drivers Installed via APT
```
sudo apt autoremove nvidia* --purge
```

Remove Previous Nvidia Drivers Installed via RUNFILE
```
sudo /usr/bin/nvidia-uninstall
```

```
curl -fSsL https://developer.download.nvidia.com/compute/cuda/repos/debian12/x86_64/3bf863cc.pub | sudo gpg --dearmor | sudo tee /usr/share/keyrings/nvidia-drivers.gpg > /dev/null 2>&1
echo 'deb [signed-by=/usr/share/keyrings/nvidia-drivers.gpg] https://developer.download.nvidia.com/compute/cuda/repos/debian12/x86_64/ /' | sudo tee /etc/apt/sources.list.d/nvidia-drivers.list
```

Update
```
apt update
apt upgrade
```

driver
```
apt install build-essential pve-headers-$(uname -r)
```

for driver
```
apt install nvidia-driver-cuda
```

for docker
```
sudo apt-get install -y nvidia-container-toolkit
```

Create the file /etc/modprobe.d/nvidia-installer-disable-nouveau.conf with the following contents:
```
# generated by nvidia-installer
blacklist nouveau
options nouveau modeset=0
```

Create the file /etc/udev/rules.d/70-nvidia.rules and add the following:
```
# /etc/udev/rules.d/70-nvidia.rules
# Create /nvidia0, /dev/nvidia1 â€¦ and /nvidiactl when nvidia module is loaded
KERNEL=="nvidia", RUN+="/bin/bash -c '/usr/bin/nvidia-smi -L && /bin/chmod 666 /dev/nvidia*'"
# Create the CUDA node when nvidia_uvm CUDA module is loaded
KERNEL=="nvidia_uvm", RUN+="/bin/bash -c '/usr/bin/nvidia-modprobe -c0 -u && /bin/chmod 0666 /dev/nvidia-uvm*'"
```

```
update-initramfs -u
reboot
```


```
ls /dev/nvidia* -l
```

```
lxc.mount.entry: /dev/dri dev/dri none bind,optional,create=dir
lxc.mount.entry: /dev/fb0 dev/fb0 none bind,optional,create=file
lxc.mount.entry: /dev/nvidia0 dev/nvidia0 none bind,optional,create=file
lxc.mount.entry: /dev/nvidiactl dev/nvidiactl none bind,optional,create=file
lxc.mount.entry: /dev/nvidia-modeset dev/nvidia-modeset none bind,optional,create=file
lxc.mount.entry: /dev/nvidia-uvm dev/nvidia-uvm none bind,optional,create=file
lxc.mount.entry: /dev/nvidia-uvm-tools dev/nvidia-uvm-tools none bind,optional,create=file
lxc.mount.entry: /dev/nvidia-uvm-tools dev/nvidia-caps/nvidia-cap1 none bind,optional,create=file
lxc.mount.entry: /dev/nvidia-uvm-tools dev/nvidia-caps/nvidia-cap2 none bind,optional,create=file
```

```
apt-get install libvulkan1
sh ./NVIDIA-Linux-x86_64-<VERSION>.run --no-kernel-module
sudo apt install software-properties-common
sudo add-apt-repository contrib
wget https://developer.download.nvidia.com/compute/cuda/repos/debian11/x86_64/cuda-keyring_1.0-1_all.deb
sudo dpkg -i cuda-keyring_1.0-1_all.deb
apt update
apt install cuda libcudnn8 libcudnn8-dev
```

```
python3 -m pip install tensorflow[and-cuda]
```
or
```
pip3 install nvidia-tensorrt cuda-python tensorflow nvidia-cudnn-cu12
```

copy all the libnvinfer* in python /usr/local/cuda
```
 cp ~/shvenv/lib64/python3.9/site-packages/tensorrt_libs/libnv* .
```

```
ln -s  libcudart.so.12.1.105  libcudart.so.11.0
ln -s libnvinfer.so.8 libnvinfer.so.7
ln -s libnvinfer_plugin.so.8 libnvinfer_plugin.so.7
ln -s libcublas.so.12.1.3.1 libcublas.so.11
ln -s libcublasLt.so.12.1.3.1 libcublasLt.so.11
ln -s libcufft.so.11.0.2.54 libcufft.so.10
ln -s  libcusparse.so.12.1.0.106 libcusparse.so.11
```

```
curl -s -L https://nvidia.github.io/nvidia-container-runtime/gpgkey | \
  sudo apt-key add -
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-container-runtime/$distribution/nvidia-container-runtime.list | \
  sudo tee /etc/apt/sources.list.d/nvidia-container-runtime.list
sudo apt-get update
```

```
LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH python3
import tensorflow as tf
tf.config.list_physical_devices('GPU')
```

```
 watch -d -n 0.5 nvidia-smi
```
